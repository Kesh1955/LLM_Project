{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM As Judge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from src.LLM_as_judge.answer import V1_EXPLANATION_ANSWER_PROMPT\n",
    "\n",
    "train_df = add_llm_explanation_column(\n",
    "    df= train_df,\n",
    "    system_prompts_dict=V1_ANSWER_SYSTEM_PROMPT,  # your prompt\n",
    "    model_name=\"gpt-4\",\n",
    "    gold_col=\"gold_answer\",\n",
    "    pred_col=\"model_answer\",\n",
    "    new_col_name=\"answer_explanation\"\n",
    ")\n",
    "\n",
    "train_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematic way to add columns and prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "system_prompts_cot = {\n",
    "    \"system_prompt\": (\n",
    "        \"You are a helpful assistant.\\n\"\n",
    "        \"Please think step-by-step to find the correct answer.\\n\"\n",
    "        \"Only provide the answer as a float, with no symbols exept for - when required.\"\n",
    "        \"Then provide only the final answer.\\n\\n\"\n",
    "        \"=== PRE TEXT ===\\n{pre_text}\\n\\n\"\n",
    "        \"=== POST TEXT ===\\n{post_text}\\n\\n\"\n",
    "        \"=== TABLE ===\\n{table}\\n\\n\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "def construct_main_messages(record: dict, system_prompts: dict) -> list:\n",
    "    \"\"\"\n",
    "    Builds the final messages by looking up a prompt in system_prompts[\"system_prompt\"],\n",
    "    then formatting placeholders {pre_text}, {post_text}, and {table} with data from record.\n",
    "    \"\"\"\n",
    "    system_prompt_template = system_prompts[\"system_prompt\"]\n",
    "    \n",
    "    system_message_content = system_prompt_template.format(\n",
    "        pre_text=record.get(\"pre_text\", \"\"),\n",
    "        post_text=record.get(\"post_text\", \"\"),\n",
    "        table=record.get(\"table\", \"\")  # always use record[\"table\"]\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message_content},\n",
    "        {\"role\": \"user\", \"content\": record.get(\"question\", \"\")}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def process_records(records, system_prompts, model_name, prompt_style):\n",
    "    \"\"\"\n",
    "    For each record, calls OpenAI and returns a DataFrame with [id, question, gold_answer, model_answer, model, prompt_style].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for r in records:\n",
    "        messages = construct_main_messages(r, system_prompts)\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=messages\n",
    "            )\n",
    "            model_answer = response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            model_answer = f\"Error: {e}\"\n",
    "\n",
    "        results.append({\n",
    "            \"id\": r[\"id\"],\n",
    "            \"question\": r[\"question\"],\n",
    "            \"gold_answer\": r[\"gold_answer\"],\n",
    "            \"table\": r[\"table\"],\n",
    "            \"model_answer\": model_answer,\n",
    "            \"model\": model_name,\n",
    "            \"prompt_style\": prompt_style,\n",
    "            \"prompt\": system_prompts,\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.models import gpt_4o_mini\n",
    "\n",
    "df_run_cot = process_records(\n",
    "    records=train_data[:2],\n",
    "    system_prompts=V1_ANSWER_SYSTEM_PROMPT,\n",
    "    model_name= gpt_4o_mini,\n",
    "    prompt_style=\"CoT\"\n",
    ")\n",
    "\n",
    "df_run_cot.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using o1 - The prompt structure is different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": f\"\"\"{Conversion_prompt}\n",
    "\n",
    "#             QUESTION:\n",
    "#             {question}\n",
    "#             \"\"\"\n",
    "\n",
    "#             # Insert json relevant data somewhere, ask the question to help it answer the questions\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "#     # 6. Call the OpenAI ChatCompletion endpoint\n",
    "#     try:\n",
    "#         response = openai.chat.completions.create(\n",
    "#             model='o1-preview',\n",
    "#             messages=messages\n",
    "\n",
    "#         )\n",
    "#         model_answer = response.choices[0].message.content\n",
    "#     except Exception as e:\n",
    "#         model_answer = f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 - Notes and questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final output - What will you show and send to Tomorro?\n",
    "\n",
    "\n",
    "# Key Q's \n",
    "\n",
    "\n",
    "# 1.  How will you run the best prompt on the Dev set (also will reduce to like ~100 samples as well given costs)\n",
    "# 1. How will you improve model performance? (criteria:systematic, repeatable, automatable???) \n",
    "# 1. How will the evals be presented? \n",
    "# 2. How will you track model and prompts?\n",
    "\n",
    "\n",
    "### In notebook\n",
    "1. Evals on 'accuracy' via exact-match with basic prompt and 4o.\n",
    "2. Evals on program execution via exact match and specified criteria with basic prompt and 4o.\n",
    "3. Improvements made via prompt-engineering (i) CoT & (ii) ReAct using 4o based on insights gathered in first experiments.\n",
    "4. Improvements made via reasoning tooling to determine what differences were possible using reasoning models.\n",
    "5. Fine-tunning to dertemine wheather further improvements were possible.\n",
    "\n",
    "\n",
    "### Submission \n",
    "- A link to a repo with the current code structure and accompanying info in README.md \n",
    "\n",
    "\n",
    "\n",
    "### Must-have's:\n",
    "1. All items in notebook\n",
    "2. Submission\n",
    "\n",
    "\n",
    "\n",
    "### Should\n",
    "1. ...\n",
    "\n",
    "\n",
    "### Could \n",
    "1. .. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
