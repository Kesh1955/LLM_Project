# **ConvFinQA Prompt Evaluation & Analysis**

This repository contains the evaluation and analysis of **ConvFinQA** dataset responses using different **LLM prompt techniques**. The project is focused on **evaluating numerical answer correctness and program equivalence**, testing few-shot, CoT, and other prompting strategies.

## **ğŸ” Project Overview**
This project explores:
- How well different **LLM prompt techniques** (e.g., Chain-of-Thought, Few-Shot) perform in **answer extraction** and **program generation**.
- The **accuracy of numerical responses** using **exact match**.
- The **equivalence of generated programs** to gold programs (ground truth).
- Using **LLMs as a judge** to evaluate program correctness.

## **ğŸ“‚ Folder Structure**

project/
â”‚â”€â”€ data/                     # Original dataset (JSON from ConvFinQA repo)
â”‚   â”‚â”€â”€ filtered_data/        # Processed dataset (filtered JSON)
â”‚   â”‚â”€â”€ answer_data/          # CSV outputs from different answer-based prompt techniques
â”‚   â”‚â”€â”€ program_data/         # CSV outputs from different program-based prompt techniques
â”‚
â”‚â”€â”€ Evaluate_metrics/         # Summarizing results
â”‚   â”‚â”€â”€ Answer.ipynb          # Analysis & findings for answer-based prompts
â”‚   â”‚â”€â”€ Program.ipynb         # Analysis & findings for program-based prompts
â”‚
â”‚â”€â”€ 01_Answer_Eval.ipynb      # Main notebook for loading & testing answers
â”‚â”€â”€ 02_Program_Eval.ipynb     # Main notebook for loading & testing programs
â”‚
â”‚â”€â”€ src/                      # Source code & system prompts
â”‚   â”‚â”€â”€ answer_prompt/        # Prompts used for extracting answers
â”‚   â”‚â”€â”€ program_prompt/       # Prompts used for generating programs
â”‚   â”‚â”€â”€ LLM_as_judge/         # Prompts for evaluating program equivalence with LLMs
â”‚
â”‚â”€â”€ README.md                 # This file
â”‚â”€â”€ .gitignore                # Files & folders to ignore in Git


 ## **ğŸ“Š Dataset**
We use **ConvFinQA**, a financial reasoning dataset that extends **FinQA** by incorporating multi-turn question-answering with numerical reasoning.

- ğŸ“œ Paper: [ACL Anthology](https://arxiv.org/abs/2210.03849)
- ğŸ“‚ Dataset Source: [ConvFinQA GitHub](https://github.com/czyssrs/ConvFinQA)


## **Running Evaluation**
Answer-Based Evaluation:
Open 01_Answer_Eval.ipynb to explore, test, and evaluate numerical answer correctness.

 ## **Program-Based Evaluation**
Open 02_Program_Eval.ipynb to test the modelâ€™s ability to generate mathematically equivalent programs.

 ## **Understanding Results**
Use Evaluate_metrics/Answer.ipynb for summary findings on answer accuracy.
Use Evaluate_metrics/Program.ipynb for insights on program correctness.

## **ğŸš€ How to Use**

### **Setup**
```bash
git clone https://github.com/Kesh1955/LLM_Project.git
cd project

Install poetry (if not already installed)
curl -sSL https://install.python-poetry.org | python3 -

Install dependencies
poetry install # This will install all dependencies listed in pyproject.toml and ensure a consistent environment using the poetry.lock file.

Activate Virtual Env manage by Poetry
poetry shell

Alternative: Using pip Instead of Poetry
If you prefer pip over Poetry, you can export the dependencies and install them manually
poetry export -f requirements.txt --output requirements.txt
pip install -r requirements.txt
```

