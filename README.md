# **ConvFinQA Prompt Evaluation & Analysis**

This repository contains the evaluation and analysis of **ConvFinQA** dataset responses using different **LLM prompt techniques**. The project is focused on **evaluating numerical answer correctness and program equivalence**, testing few-shot, CoT, and other prompting strategies.

## **ğŸ” Project Overview**
This project explores:
- How well different **LLM prompt techniques** (e.g., Chain-of-Thought, Few-Shot) perform in **answer extraction** and **program generation**.
- The **accuracy of numerical responses** using **exact match**.
- The **equivalence of generated programs** to gold programs (ground truth).
- Using **LLMs as a judge** to evaluate program correctness.

## **ğŸ“‚ Folder Structure**

project/ â”‚â”€â”€ data/ # Original dataset (JSON from ConvFinQA repo) â”‚â”€â”€ filtered_data/ # Processed dataset (filtered JSON) â”‚â”€â”€ answer_data/ # CSV outputs from different answer-based prompt techniques â”‚â”€â”€ program_data/ # CSV outputs from different program-based prompt techniques â”‚â”€â”€ Evaluate_metrics/ # Summarizing results â”‚ â”œâ”€â”€ Answer.ipynb # Analysis & findings for answer-based prompts â”‚ â”œâ”€â”€ Program.ipynb # Analysis & findings for program-based prompts â”‚â”€â”€ 01_Answer_Eval.ipynb # Main notebook for loading & testing answers â”‚â”€â”€ 02_Program_Eval.ipynb # Main notebook for loading & testing programs â”‚â”€â”€ src/ # Source code & system prompts â”‚ â”œâ”€â”€ answer_prompt/ # Prompts used for extracting answers â”‚ â”œâ”€â”€ program_prompt/ # Prompts used for generating programs â”‚ â”œâ”€â”€ LLM_as_judge/ # Prompts for evaluating program equivalence with LLMs â”‚â”€â”€ README.md # This file â”‚â”€â”€ .gitignore # Files & folders to ignore in Git

<!-- 'Evaluation_metrics' - has 2 notebooks:
    (i) Answer.ipynb - Calling in csvs from 'answer_data' to more neatly summarise findings and limitations
    (ii) Program.ipynb - Calling in csvs from 'program_data' to more neatly summarise findings and limitations

'01_Answer_Eval.ipynb' & '02_Program_Eval.ipynb' - Most of the work to load, explore, test and eval data is in here. More detailed information at each step in the process.

'data' - Original json data from ConvFinQA Github: https://github.com/czyssrs/ConvFinQA. Paper is here: https://arxiv.org/abs/2210.03849
'filtered_data' -  is the new json created once I've filtered out unecessary keys 
'program_data' - is the csv outputs from analyses and different prompt techniques
'answer_data' - is the csv outputs from analyses and different prompt techniques

'src'
'answer_prompt' - System prompts from 'answer'
'program_prompt - System prompts from 'program'
'LLM_as_judge' - System prompts from using LLM as a judge -->




 ## **ğŸ“Š Dataset**
We use **ConvFinQA**, a financial reasoning dataset that extends **FinQA** by incorporating multi-turn question-answering with numerical reasoning.

- ğŸ“œ Paper: [ACL Anthology](https://arxiv.org/abs/2210.03849)
- ğŸ“‚ Dataset Source: [ConvFinQA GitHub](https://github.com/czyssrs/ConvFinQA)

## **ğŸš€ How to Use**
### **1ï¸âƒ£ Setup**
```bash
git clone https://github.com/your-repo-name.git ###Â To-do 
cd project
pip install -r requirements.txt  # (if dependencies are needed) ###Â to-do 


2ï¸âƒ£ Running Evaluation
Answer-Based Evaluation:
Open 01_Answer_Eval.ipynb to explore, test, and evaluate numerical answer correctness.

Program-Based Evaluation:
Open 02_Program_Eval.ipynb to test the modelâ€™s ability to generate mathematically equivalent programs.
3ï¸âƒ£ Understanding Results
Use Evaluate_metrics/Answer.ipynb for summary findings on answer accuracy.
Use Evaluate_metrics/Program.ipynb for insights on program correctness.
