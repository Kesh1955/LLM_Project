# **ConvFinQA Prompt Evaluation & Analysis**

This repository contains the evaluation and analysis of **ConvFinQA** dataset responses using different **LLM prompt techniques**. The project is focused on **evaluating numerical answer correctness and program equivalence**, testing few-shot, CoT, and other prompting strategies.

## **üîç Project Overview**
This project explores:
- How well different **LLM prompt techniques** (e.g., Chain-of-Thought, Few-Shot) perform in **answer extraction** and **program generation**.
- The **accuracy of numerical responses** using **exact match**.
- The **equivalence of generated programs** to gold programs (ground truth).
- Using **LLMs as a judge** to evaluate program correctness.

## **üìÇ Folder Structure**


- **project/**
  - `data/` - Original dataset (JSON from ConvFinQA repo)
    - `filtered_data/` - Processed dataset (filtered JSON)
    - `answer_data/` - CSV outputs from different answer-based prompt techniques
    - `program_data/` - CSV outputs from different program-based prompt techniques
  - `Evaluate_metrics/` - Summarizing results
    - `Answer.ipynb` - Analysis & findings for answer-based prompts
    - `Program.ipynb` - Analysis & findings for program-based prompts
  - `project/`
    - `01_Answer_Eval.ipynb` - Main notebook for loading & testing answers
    - `02_Program_Eval.ipynb` - Main notebook for loading & testing programs
  - `src/` - Source code & system prompts
    - `answer_prompt/` - Prompts used for extracting answers
    - `program_prompt/` - Prompts used for generating programs
    - `LLM_as_judge/` - Prompts for evaluating program equivalence with LLMs
  - `README.md` - This file
  - `.gitignore` - Files & folders to ignore in Git


 ## **üìä Dataset**
We use **ConvFinQA**, a financial reasoning dataset that extends **FinQA** by incorporating multi-turn question-answering with numerical reasoning.

- üìú Paper: [ACL Anthology](https://arxiv.org/abs/2210.03849)
- üìÇ Dataset Source: [ConvFinQA GitHub](https://github.com/czyssrs/ConvFinQA)


## **Running Evaluation**
Answer-Based Evaluation:
Open 01_Answer_Eval.ipynb to explore, test, and evaluate numerical answer correctness.

 ## **Program-Based Evaluation**
Open 02_Program_Eval.ipynb to test the model‚Äôs ability to generate mathematically equivalent programs.

 ## **Understanding Results**
Use Evaluate_metrics/Answer.ipynb for summary findings on answer accuracy.
Use Evaluate_metrics/Program.ipynb for insights on program correctness.

## **üöÄ How to Use**

### **Setup**
```bash
git clone https://github.com/Kesh1955/LLM_Project.git
cd project

Install poetry (if not already installed)
curl -sSL https://install.python-poetry.org | python3 -

Install dependencies
poetry install # This will install all dependencies listed in pyproject.toml 
               #¬†and ensure a consistent environment using the poetry.lock file.

Activate Virtual Env manage by Poetry
poetry shell

Alternative: Using pip Instead of Poetry
If you prefer pip over Poetry, you can export the dependencies and install them manually
poetry export -f requirements.txt --output requirements.txt
pip install -r requirements.txt
```

